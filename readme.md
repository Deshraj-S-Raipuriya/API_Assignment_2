python -m venv venv

curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py

python get-pip.py

pip install -r  requirements.txt  


# Get Api keys
# Open Api key

https://platform.openai.com/docs/overview

#hugging face portal
https://huggingface.co/


# prepare env
pip install transformers datasets accelerate

# example trainer script (train_intent.py) - use Hugging Face 
python train_intent.py --model distilbert-base-uncased --data my_intent_dataset.csv


# Step-by-Step Setup for Grafana Dashboard (with Prometheus)
# Step 1 â€” Install and Start Prometheus

    #Option 1: via Docker (recommended)

    docker run -d --name=prometheus -p 9090:9090 prom/prometheus


    # Option 2: manual setup

    Download from https://prometheus.io/download/

    # Extract and edit prometheus.yml:

    scrape_configs:
    - job_name: "smartassist"
        static_configs:
        - targets: ["localhost:8000"]

# Step 2 â€” Install and Start Grafana

Using Docker

docker run -d --name=grafana -p 3000:3000 grafana/grafana


Default Login:

Username: admin

Password: admin

âš™ï¸ Step 3 â€” Expose Metrics from Python Services

Each sub-task (Chat, Intent, RAG, CV, ASR) should expose metrics using prometheus_client.

Install it:

pip install prometheus_client


In each service (or a combined monitor.py), add:

from prometheus_client import Counter, Histogram, start_http_server
import time, random

# Start metrics server (e.g., http://localhost:8000/metrics)
start_http_server(8000)

REQUEST_COUNT = Counter("requests_total", "Total number of requests", ["service"])
REQUEST_ERRORS = Counter("requests_errors_total", "Total errors", ["service"])
REQUEST_LATENCY = Histogram("request_latency_seconds", "Latency per service", ["service"])

def simulate_request(service):
    start = time.time()
    try:
        # simulate work
        time.sleep(random.uniform(0.2, 1.5))
        if random.random() < 0.1:
            raise Exception("Simulated error")
        latency = time.time() - start
        REQUEST_LATENCY.labels(service).observe(latency)
        REQUEST_COUNT.labels(service).inc()
    except Exception:
        REQUEST_ERRORS.labels(service).inc()

if __name__ == "__main__":
    print("ğŸš€ Metrics server running at http://localhost:8000/metrics")
    while True:
        for s in ["chat", "intent", "rag", "cv", "asr"]:
            simulate_request(s)


âœ… This exposes metrics like:

# HELP request_latency_seconds Latency per service
# TYPE request_latency_seconds histogram
request_latency_seconds_bucket{service="chat",le="0.5"} 2.0

ğŸ“Š Step 4 â€” Add Prometheus as Data Source in Grafana

Open Grafana â†’ http://localhost:3000

Go to Settings â†’ Data Sources â†’ Add data source

Choose Prometheus

Set URL â†’ http://localhost:9090

Click Save & Test

ğŸ§© Step 5 â€” Create Dashboard Panels

Create new dashboard â†’ Add panels like:

ğŸ”¹ Panel 1: Request Latency

Query:

histogram_quantile(0.95, sum(rate(request_latency_seconds_bucket[1m])) by (le, service))


Title: P95 Latency per Service

Visualization: Line chart

ğŸ”¹ Panel 2: Request Count

Query:

rate(requests_total[1m])


Title: Requests per Service

Visualization: Bar chart

ğŸ”¹ Panel 3: Error Rate

Query:

(sum(rate(requests_errors_total[1m])) by (service)) 
/ (sum(rate(requests_total[1m])) by (service))


Title: Error Rate (%)

Visualization: Time series or gauge

ğŸ”¹ Panel 4: Service Heatmap

Query:

sum by (service) (rate(request_latency_seconds_sum[1m]))


Title: Service Load

Visualization: Heatmap

ğŸ–¼ï¸ Example Layout (student-style report diagram)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SmartAssist Metrics Dashboard â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ P95 Latency per Service  ]  â”‚  âŸ¶  line chart
â”‚  [ Error Rate Gauge         ]  â”‚  âŸ¶  red if >10%
â”‚  [ Request Count Histogram   ] â”‚  âŸ¶  volume visualization
â”‚  [ Service Heatmap (Load)   ]  â”‚  âŸ¶  per-service traffic
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ˆ Step 6 â€” (Optional) Simulate Real Data from Your AI Tasks

Each module (chat, intent, etc.) should push metrics using the same labels:

REQUEST_LATENCY.labels("chat").observe(duration)
REQUEST_ERRORS.labels("chat").inc()


This allows the Grafana dashboard to automatically update with your modelâ€™s live data.

âœ… Step 7 â€” Snapshot for Submission

Once graphs show data:

Click Share â†’ Snapshot â†’ Create Public Snapshot

Copy snapshot link or export as PNG.

Add to your report with caption:

â€œFigure X: Grafana Dashboard showing model latency and error metrics for SmartAssist pipeline.â€